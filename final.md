* GPU是如何做矩阵乘法的；
* GPU做运算的效率受到哪些因素的制约（想写出充分发挥GPU能力的程序，需要从哪些方面优化）；
* 如何实际测试一个GPU程序的性能；
* Triton语言相比CUDA，优缺点是什么；
* 使用Triton编写一个矩阵乘法算法，针对多种不同规模的矩阵乘，性能要求不低于pytorch自带的矩阵乘法实现。记录下每一步优化过程。

## 1 GPU是如何做矩阵乘法

### 1-1 矩阵乘法是什么？

矩阵A(M行K列) 乘 矩阵B(K行N列) 等于 矩阵C(M行N列)；矩阵A中第m行的元素和矩阵B中第n列的元素对应相乘，再对这些乘积求和就是矩阵C中第m行n列的元素；

输入：矩阵A(M行K列) 和 矩阵B(K行N列)；输出：矩阵C(M行N列)；

程序需要做的事：求出矩阵C中每个元素的值[m,n] -> 对于k从0到K-1，求A[m][k] 乘 B[k][n]，最终把这些乘积相加，即得到C[m][n]。

### 1-2 CPU如何做矩阵乘法？

串行实现：最直接的是三重循环，依次计算矩阵C的每个元素；

并行化思路：矩阵C的各元素计算彼此独立，天然适合并行。常见做法包括：

* 多线程（如 OpenMP）：把 M×N 个输出元素的计算分给不同线程；
* 分布式（如 MPI）：把矩阵分块到多台机器协同计算。

### 1-3 GPU如何做矩阵乘法？

CPU追求低延迟与强单核，核心复杂（分支预测、乱序、多级缓存），面积和功耗高，难以大规模堆核，整体并行度受限；

GPU追求高吞吐，执行单元更简单、易于堆叠成千上万的并行通道，并行度显著更高，更适合大规模并行计算；

## 2 GPU做运算的效率收到哪些因素的制约

GPU擅长大量线程并行计算，但在计算开始前，需要先从显存或更低层存储中加载数据。运算效率很大程度上依赖于所需数据是否已提前放入共享内存或寄存器等高速存储中。

* 把数据尽早、成批搬到更近的存储层（寄存器/共享内存/缓存），并尽量重复利用；
* 用更多并发在等待期间做别的活儿，隐藏内存延迟。

这里有一些优化思路(NVIDIA GPU)：

* 内存合并：同一warp内的线程若访问连续且对齐的地址，硬件能把它们合并成更少的内存事务；否则会拆成多次事务，放大延迟与带宽占用；
* 分块(tiled)：为了提高数据复用性，可以对要计算的任务进行分块处理；
* 常量内存：对只读且被所有线程统一访问的数据(比如卷积核)，可以放在常量内存(constant memory)里面；
* 延迟隐藏：启动kernel函数时可以设置远超物理核心数的线程，这样当一个warp在等待数据时，GPU调度器会立刻切换到另一个就绪的warp去执行计算；
* 寄存器使用限制：寄存器是线程私有，如果一个线程使用过多的寄存器，会导致并行化程度不高，因为一个SM上的寄存器数是有限的，当单个线程使用过多寄存器时，意味着线程数会减少；
* 分支发散：同一个warp(32个线程)共用一个程序计数器，这意味着同一时刻，一个warp内的线程一定是在执行同一条指令，如果要执行的程序存在分支语句，且同一warp内有两组线程分别走向不同的分支，那么总的执行市场并不是执行时间最长的那个分支，而是两个分支执行时间的总和，即两个分支会顺序执行；这是通过分支掩码来实现的，管理哪个线程活跃/屏蔽；尽量让分支条件与线程组织一致（使warp内走同一分支）；
* 同步：当多个线程需要更新同一个内存地址时(比如累加到一个全局变量)，需要通过原子操作来保证正确性，会导致串行化与争用。

数据应该"取得对"、"取得齐"、"取得早"；取完之后应该"多次用"、“少同步"；同时用足够的并发把延迟藏起来。

## 3 如何实际测试一个GPU程序

首先使用Nsight System从系统层面分析程序，优化不必要同步、数据传输等，接下来使用Nsight Compute对kernel进行分析和优化。

Nsight System：

* 支持NVTX插桩，方便定位性能瓶颈；
* 关注API、kernel函数、内存操作等统计信息：
  * API：cudaDeviceSynchronize、cudaLaunchKernel等；
  * kernel函数：执行时间；
  * 内存操作：host和device之间的数据搬运次数和时长；

Nsight Compute：

* 支持多个event, metric, section：
  * event：硬件计数，如cache miss等；
  * metric：由具有相似特征event计算得到，如gld_efficiency等；
  * section：由具有相似特征metric计算得到，如memory会涉及global memory、shared memory等；
* 判断是compute/memory/latency bound中的哪一种，查看SOL Section；

![1756540258699](image/final/1756540258699.png)

* Compute Workload Analysis：各个pipeline的利用率，可以定位是卡在哪个计算环节了(fma, lsu等\)；

  ![1756607081930](image/final/1756607081930.png)
* Scheduler Statistics Analysis：反应每次调度发射的warp数占理论值的百分比；
  一个warp被阻塞而不能发射的原因：1.内存依赖；2.执行依赖；3.pipeline无空闲等

  ![1756607556655](image/final/1756607556655.png)
* Warp State Statistics Analysis：反映了warp被stall的原因；
  stall long scoreboard：访问全局内存；warp cycles per issued instruction：平均每发射一条指令需要等待多少个cycle

  ![1756607861748](image/final/1756607861748.png)
* Memory Wordload Analysis：L1/L2 命中率、带宽利用率、bank conflicts等；

  ![1756608467265](image/final/1756608467265.png)
* Launch Statistics Analysis：每个线程用了多少寄存器，共享内存的访问情况等；
  由此可以调整执行配置来进行优化··

  ![1756609093936](image/final/1756609093936.png)
* Source Conters Statistics Analysis：哪条指令执行次数最多、哪里出现了非合并访问等；

## 4 Triton语言相比CUDA，优缺点是什么

Triton开源，CUDA闭源；->开源相较于闭源有啥优势？有啥劣势？

Triton一般集成在python中，CUDA集成在C/C++中；->有啥优势，有啥劣势？

Triton程序可以让程序员专注于算法实现，而底层配置/优化由triton-jit编译器来完成，CUDA程序需要程序员在设计算法的同时兼顾执行配置、数据同步等操作；->优势和劣势？

* 开源(Triton) vs 闭源(CUDA)：
  * Triton透明可定制，并且支持跨架构；
  * CUDA生态成熟，但可移植性不强；
  * Triton作为开源项目，具备透明、可定制、无锁定等优势，但在稳定性、官方支持和新硬件首发适配上不及CUDA；CUDA闭源但成熟稳健、工具链完备、性能领先，相应的代价是厂商锁定与不可定制内部实现。
* Python(Triton) vs C/C++(CUDA)：
  * Triton语法简洁，可以和pytorch等无缝衔接，Triton-jit编译器自动调优；低层级调试工具不足，源码级映射到PTX/SASS不如C/C++直观；
  * CUDA可精细把握内存分配、异步流、共享内存使用等，使程序达到极致性能；开发成本高，开发效率不如Triton；
  * Triton通常嵌入在Python生态中，开发效率高、与深度学习框架衔接自然；CUDA主要面向C/C++，强调可控性。
* 抽象度高(Triton) vs 抽象度低(CUDA)：
  * Triton可以用更快的时间、更少的代码量写出和CUDA程序性能接近的程序，使程序员更关注算法实现；
  * CUDA程序可以最大化利用硬件，除了算法之外，还需要关注内存访问等细节；
  * Triton让开发者更专注于算法本身，把调度与优化交给编译器；CUDA 要求同时管理执行配置、内存与同步等细节，开发成本更高，但能换取极致性能与更强的确定性。

## 5 使用Triton编写一个矩阵乘法算法，针对多种不同规模的矩阵乘，性能要求不低于pytorch自带的矩阵乘法实现

![1756605675803](image/final/1756605675803.png)

注：矩阵A(M行K列) 乘 矩阵B(K行N列)，M=K=N，从256到4096，每次步长128。

* 对结果矩阵进行分块计算；
* 对结果矩阵中的块计算顺序：

![1756605930514](image/final/1756605930514.png)

* autotune的config调试。
