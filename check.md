GPU是如何做矩阵乘法的

> GPU通过启动大量线程，让每个线程独立计算输出矩阵中的一个元素，实现并行计算。

GPU做运算的效率受到哪些因素的制约（想写出充分发挥GPU能力的程序，需要从哪些方面优化）

> GPU可以启动大量线程用于计算，关键在于是否能为这些线程及时提供足够量的数据。
>
> * coalescing memory：确保一个Warp（32个线程）的线程在访问全局内存时，请求的地址是连续的；
> * tiled：对要计算的任务进行分块；
> * constant memory：对只读且被所有线程统一访问的数据(如卷积核)，放在constant memory里面；
> * latency hiding：启动远超物理核心数量的线程，当一个Warp在等待数据时，GPU调度器会立刻切换到另一个就绪的Warp去执行计算；
> * 执行配置：块数最好是SM个数的整数倍，块中线程数最好是warp中线程数的整数倍；
> * 寄存器使用限制：寄存器是线程私有，但数量有限，使用时应限制；
> * branch divergence：同一warp内的线程公用一个pc，故所有分支路径会串行执行，通过分支掩码来管理哪个线程活跃/屏蔽；
> * 同步：当多个线程需要更新同一个内存地址（如累加到一个全局变量）时，需要使用原子操作来保证正确性；

如何实际测试一个GPU程序的性能？

> Nsight Compute：
>
> * Compute(SM)；SM利用率；
> * Memort(DRAM)：DRAM利用率，如果这个值高，但是SM利用率不高，说明程序卡在内存瓶颈了；
> * Wavefront Publishling Stall：指令发射停滞率，值越高，说明前端指令调度出问题；
> * 实际活跃warp数占最大可活跃warp数：如果较低，说明寄存器使用过多/shared memory使用过多，限制了SM上能同时驻留的线程块数。
> * Bytes per Request：DRAM每次请求的字节数，如果值很小，说明coalescing memory差；
> * DRAM Throughput：显存吞吐量占理论峰值百分比；
> * L2 缓存命中率：反映了数据局部性的利用程度；
> * Shared Memory Bank Conflicts：共享内存冲突，多个线程同时访问同一个bank的不同地址，导致并行访问串行化；可以用padding来缓解；
> * 还有数据搬运耗时(Device2Host)等指标。

Triton语言相比CUDA，优缺点是什么。

> 大体上：Triton高层级，面向算法；CUDA低层级，接近硬件。->Triton开发效率更高，但CUDA更适合开发极致性能的程序。
>
> Triton编程省劲的原因：Triton编译器将原本在CUDA编程中需要程序员手动编写的一些操作给接管，程序员在写Triton程序时可以很容易，但这也导致如果想做细致的优化，可能不如CUDA。
>
> 可移植性：Triton支持多个GPU并且可以autotune执行配置；而CUDA编程中，对于不同的问题、GPU架构可能需要不同的执行配置。
>
> 生态：CUDA更胜一筹，Triton目前仍在高速发展。

使用Triton编写一个矩阵乘法算法，针对多种不同规模的矩阵乘，性能要求不低于pytorch自带的矩阵乘法实现。记录下每一步优化过程。

> 实现了三个版本：代码在[这](0821/pytorch_triton.ipynb)。
>
> * 逐元素矩阵乘：对于结果矩阵C中的每一个元素，并行计算；
> * 分块矩阵乘v1：对于结果矩阵C中的元素进行分块，每块由一个program处理，提高了数据复用率；
> * 分块矩阵乘v2：v1在对结果矩阵C中的块计算时，是行优先，这里采用分组，即把结果矩阵m*n块分为多组，每组包含x行块，在组内采用列优先计算，进一步提高数据复用率；
>
> 正确性方面已验证，性能方面仍不如pytorch自带的矩阵乘法，可能是因为我给的autotune的configs太少了，还需调试。
