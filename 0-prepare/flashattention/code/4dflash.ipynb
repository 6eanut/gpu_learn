{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大绝对误差: 6.556510925292969e-07\n",
      "是否近似相等: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@triton.jit\n",
    "def flash_attention_4d(\n",
    "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
    "    num_heads, seq_len, head_dim: tl.constexpr,\n",
    "    stride_qb, stride_qh, stride_qm, stride_qd,\n",
    "    stride_kb, stride_kh, stride_km, stride_kd,\n",
    "    stride_vb, stride_vh, stride_vm, stride_vd,\n",
    "    stride_ob, stride_oh, stride_om, stride_od,\n",
    "    BLOCK_M: tl.constexpr,  # Br: Q的分块大小\n",
    "    BLOCK_N: tl.constexpr,  # Bc: K,V的分块大小\n",
    "):\n",
    "    # 获取当前处理的batch和head索引\n",
    "    pid_bh = tl.program_id(0)  # batch和head的组合索引\n",
    "    pid_m = tl.program_id(1)   # 序列维度的块索引\n",
    "    \n",
    "    # 计算batch和head索引\n",
    "    batch_idx = pid_bh // num_heads\n",
    "    head_idx = pid_bh % num_heads\n",
    "    \n",
    "    # 计算Q块的起始位置\n",
    "    start_m = pid_m * BLOCK_M\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M)\n",
    "    offs_d = tl.arange(0, head_dim)\n",
    "    \n",
    "    # 计算基础指针偏移\n",
    "    q_base = q_ptr + batch_idx * stride_qb + head_idx * stride_qh\n",
    "    k_base = k_ptr + batch_idx * stride_kb + head_idx * stride_kh\n",
    "    v_base = v_ptr + batch_idx * stride_vb + head_idx * stride_vh\n",
    "    o_base = o_ptr + batch_idx * stride_ob + head_idx * stride_oh\n",
    "    \n",
    "    # 初始化状态变量\n",
    "    m_prev = tl.full((BLOCK_M,), float('-inf'), dtype=tl.float32)\n",
    "    l_prev = tl.zeros((BLOCK_M,), dtype=tl.float32)\n",
    "    acc = tl.zeros((BLOCK_M, head_dim), dtype=tl.float32)\n",
    "    \n",
    "    # 加载Q分块\n",
    "    q_mask = offs_m[:, None] < seq_len\n",
    "    q = tl.load(\n",
    "        q_base + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qd,\n",
    "        mask=q_mask,\n",
    "        other=0.0\n",
    "    )\n",
    "    \n",
    "    # 遍历K,V的分块\n",
    "    for start_n in range(0, seq_len, BLOCK_N):\n",
    "        offs_n = start_n + tl.arange(0, BLOCK_N)\n",
    "        mask_n = offs_n < seq_len\n",
    "        \n",
    "        # 加载K分块\n",
    "        k = tl.load(\n",
    "            k_base + offs_n[:, None] * stride_km + offs_d[None, :] * stride_kd,\n",
    "            mask=mask_n[:, None],\n",
    "            other=0.0\n",
    "        )\n",
    "        \n",
    "        # 加载V分块\n",
    "        v = tl.load(\n",
    "            v_base + offs_n[:, None] * stride_vm + offs_d[None, :] * stride_vd,\n",
    "            mask=mask_n[:, None],\n",
    "            other=0.0\n",
    "        )\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        s = tl.dot(q, k.T)\n",
    "        s *= 1.0 / tl.sqrt(tl.cast(head_dim, s.dtype))\n",
    "        \n",
    "        # 应用mask\n",
    "        s = tl.where(mask_n[None, :], s, float('-inf'))\n",
    "        \n",
    "        # 在线Softmax\n",
    "        m_current = tl.maximum(tl.max(s, axis=1), m_prev)\n",
    "        exp_m_prev = tl.exp(m_prev - m_current)\n",
    "        exp_s = tl.exp(s - m_current[:, None])\n",
    "        l_current = exp_m_prev * l_prev + tl.sum(exp_s, axis=1)\n",
    "        \n",
    "        # 更新累加器\n",
    "        # 把 算法中的1D张量转换成对角矩阵后做矩阵乘 变为 reshape成2D张量后再做逐元素相乘时进行广播\n",
    "        acc = acc * exp_m_prev[:, None] + tl.dot(exp_s, v)\n",
    "        \n",
    "        # 更新状态\n",
    "        m_prev = m_current\n",
    "        l_prev = l_current\n",
    "    \n",
    "    # 归一化并写入结果, 思路和前面的更新累加器类似\n",
    "    acc = acc / l_prev[:, None]\n",
    "    tl.store(\n",
    "        o_base + offs_m[:, None] * stride_om + offs_d[None, :] * stride_od,\n",
    "        acc,\n",
    "        mask=q_mask\n",
    "    )\n",
    "\n",
    "def flash_attn_func(q, k, v):\n",
    "    \"\"\"\n",
    "    仿照flash_attn库的接口，支持4D张量输入\n",
    "    \n",
    "    Arguments:\n",
    "        q: (batch_size, seq_len, num_heads, head_dim)\n",
    "        k: (batch_size, seq_len, num_heads_k, head_dim)\n",
    "        v: (batch_size, seq_len, num_heads_k, head_dim)\n",
    "    \n",
    "    Returns:\n",
    "        out: (batch_size, seq_len, num_heads, head_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, seq_len, num_heads, head_dim = q.shape\n",
    "    \n",
    "    # 创建输出张量\n",
    "    o = torch.empty_like(q)\n",
    "    \n",
    "    # 计算分块大小\n",
    "    block_m, block_n = calculate_block_sizes(head_dim)\n",
    "    \n",
    "    # 设置grid大小\n",
    "    grid = (batch_size * num_heads, triton.cdiv(seq_len, block_m))\n",
    "    \n",
    "    # 调用kernel\n",
    "    flash_attention_4d[grid](\n",
    "        q, k, v, o,\n",
    "        num_heads, seq_len, head_dim,\n",
    "        q.stride(0), q.stride(2), q.stride(1), q.stride(3),\n",
    "        k.stride(0), k.stride(2), k.stride(1), k.stride(3),\n",
    "        v.stride(0), v.stride(2), v.stride(1), v.stride(3),\n",
    "        o.stride(0), o.stride(2), o.stride(1), o.stride(3),\n",
    "        BLOCK_M=block_m,\n",
    "        BLOCK_N=block_n,\n",
    "    )\n",
    "    \n",
    "    return o\n",
    "\n",
    "def calculate_block_sizes(head_dim, sram_size_kb=48):\n",
    "    sram_size = sram_size_kb * 1024\n",
    "    bytes_per_elem = 4\n",
    "    \n",
    "    bc = sram_size // (4 * head_dim * bytes_per_elem)\n",
    "    br = min(bc, head_dim)\n",
    "    \n",
    "    bc = 2 ** (bc.bit_length() - 1) if bc > 0 else 1\n",
    "    \n",
    "    br = min(bc, head_dim)\n",
    "    br = 2 ** (br.bit_length() - 1) if br > 0 else 1\n",
    "    \n",
    "    return br, bc\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 测试4D输入\n",
    "    batch_size = 2\n",
    "    seq_len = 512\n",
    "    num_heads = 8\n",
    "    head_dim = 64\n",
    "    \n",
    "    # 创建4D输入\n",
    "    q = torch.randn(batch_size, seq_len, num_heads, head_dim, \n",
    "                    device=\"cuda\", dtype=torch.float32)\n",
    "    k = torch.randn(batch_size, seq_len, num_heads, head_dim, \n",
    "                    device=\"cuda\", dtype=torch.float32)\n",
    "    v = torch.randn(batch_size, seq_len, num_heads, head_dim, \n",
    "                    device=\"cuda\", dtype=torch.float32)\n",
    "    \n",
    "    # 使用flash_attn_func\n",
    "    output = flash_attn_func(q, k, v)\n",
    "    \n",
    "    # 与PyTorch标准注意力比较\n",
    "    def torch_attention_4d(q, k, v):\n",
    "        batch_size, seq_len, num_heads, head_dim = q.shape\n",
    "        \n",
    "        # (batch_size, seq_len, num_heads, head_dim)\n",
    "        # (batch_size, num_heads, seq_len, head_dim)\n",
    "        # (batch_size * num_heads, seq_len, head_dim)\n",
    "        q = q.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)\n",
    "        v = v.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # 计算注意力\n",
    "        scores = torch.bmm(q, k.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.bmm(attn_weights, v)\n",
    "        \n",
    "        # 重塑回原始形状\n",
    "        output = output.reshape(batch_size, num_heads, seq_len, head_dim)\n",
    "        output = output.transpose(1, 2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # 比较结果\n",
    "    output_torch = torch_attention_4d(q, k, v)\n",
    "    print(f\"最大绝对误差: {(output - output_torch).abs().max().item()}\")\n",
    "    print(f\"是否近似相等: {torch.allclose(output, output_torch, atol=1e-2, rtol=1e-2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
