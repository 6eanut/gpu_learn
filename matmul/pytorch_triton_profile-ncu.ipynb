{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8278e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "!pip install numpy==1.23.3 pandas triton matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf50c43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "      a_ptr, b_ptr, c_ptr,\n",
    "      M, N, K,\n",
    "      stride_am, stride_ak,\n",
    "      stride_bk, stride_bn,\n",
    "      stride_cm, stride_cn,\n",
    "      BLOCK_SIZE_M: tl.constexpr,\n",
    "      BLOCK_SIZE_N: tl.constexpr,\n",
    "      BLOCK_SIZE_K: tl.constexpr,\n",
    "      GROUP_SIZE_M: tl.constexpr\n",
    "      ):\n",
    "  # program <-> one block of matrix c <-> pid_m, pid_n, BLOCK_SIZE_M, BLOCK_SIZE_N\n",
    "  pid = tl.program_id(axis=0)\n",
    "  # how many blocks in m and n ?\n",
    "  num_pid_m = tl.cdiv(M,BLOCK_SIZE_M)\n",
    "  num_pid_n = tl.cdiv(N,BLOCK_SIZE_N)\n",
    "  # how many blocks in a group ?\n",
    "  num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "  # which group current program exists in ?\n",
    "  group_id = pid // num_pid_in_group\n",
    "  # start of the group ?\n",
    "  first_pid_m = group_id * GROUP_SIZE_M\n",
    "  # if the number of blocks in m isn't divisible by GROUP_SIZE_M, then the last group has not GROUP_SIZE_M lines\n",
    "  group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "  # the idx of block in group\n",
    "  pid_in_group = pid % num_pid_in_group\n",
    "  # pid_m\n",
    "  pid_m = first_pid_m + pid_in_group % group_size_m\n",
    "  # pid_n\n",
    "  pid_n = pid_in_group // group_size_m\n",
    "\n",
    "  # element in block\n",
    "  offs_am = (pid_m*BLOCK_SIZE_M + tl.arange(0,BLOCK_SIZE_M))%M\n",
    "  offs_bn = (pid_n*BLOCK_SIZE_N + tl.arange(0,BLOCK_SIZE_N))%N\n",
    "  offs_k = tl.arange(0,BLOCK_SIZE_K)\n",
    "  a_ptrs = a_ptr + (offs_am[:,None]*stride_am + offs_k[None,:]*stride_ak)\n",
    "  b_ptrs = b_ptr + (offs_k[:,None]*stride_bk + offs_bn[None,:]*stride_bn)\n",
    "\n",
    "  # do the multiply\n",
    "  accumulator = tl.zeros((BLOCK_SIZE_M,BLOCK_SIZE_N),dtype=tl.float32)\n",
    "  for k in range(0,tl.cdiv(K,BLOCK_SIZE_K)):\n",
    "    a=tl.load(a_ptrs, mask=offs_k[None,:]<K-k*BLOCK_SIZE_K,other=0.0)\n",
    "    b=tl.load(b_ptrs, mask=offs_k[:,None]<K-k*BLOCK_SIZE_K,other=0.0)\n",
    "    accumulator += tl.dot(a,b)\n",
    "    a_ptrs+=BLOCK_SIZE_K*stride_ak\n",
    "    b_ptrs+=BLOCK_SIZE_K*stride_bk\n",
    "  c = accumulator.to(tl.float16)\n",
    "\n",
    "  # write the result in c\n",
    "  offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0,BLOCK_SIZE_M)\n",
    "  offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0,BLOCK_SIZE_N)\n",
    "  c_ptrs = c_ptr+stride_cm*offs_cm[:,None] + stride_cn * offs_cn[None,:]\n",
    "  c_mask =(offs_cm[:,None]<M)&(offs_cn[None,:]<N)\n",
    "  tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "def matmul(a, b):\n",
    "  assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "  assert a.is_contiguous(), \"Matrix A bust be contiguous\"\n",
    "  M, K = a.shape\n",
    "  K, N = b.shape\n",
    "  c = torch.empty((M,N),device=a.device, dtype=torch.float16)\n",
    "  grid = lambda META: (\n",
    "      triton.cdiv(M, 128) * triton.cdiv(N, 128),\n",
    "  )\n",
    "  matmul_kernel[grid](a, b, c, M, N, K, \n",
    "                    a.stride(0), a.stride(1), \n",
    "                    b.stride(0), b.stride(1), \n",
    "                    c.stride(0), c.stride(1),\n",
    "                    BLOCK_SIZE_M=128, \n",
    "                    BLOCK_SIZE_N=128, \n",
    "                    BLOCK_SIZE_K=32, \n",
    "                    GROUP_SIZE_M=8, \n",
    "                    num_warps=4)\n",
    "  return c\n",
    "  \n",
    "DEVICE = 'cuda'\n",
    "\n",
    "ref_lib = 'cuBLAS'\n",
    "\n",
    "a = torch.randn((1024, 1024), device=DEVICE, dtype=torch.float16)\n",
    "b = torch.randn((1024, 1024), device=DEVICE, dtype=torch.float16)\n",
    "\n",
    "c_triton = matmul(a, b)\n",
    "\n",
    "c_ref = torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ae8f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nsys profile --stats=true python ./test.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
