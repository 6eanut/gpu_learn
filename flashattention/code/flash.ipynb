{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "U3nQMf-ky-vc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch==2.7.1 in /root/miniforge3/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /root/miniforge3/lib/python3.10/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio==2.7.1 in /root/miniforge3/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /root/miniforge3/lib/python3.10/site-packages (from torch==2.7.1) (3.3.1)\n",
      "Requirement already satisfied: numpy in /root/miniforge3/lib/python3.10/site-packages (from torchvision==0.22.1) (1.23.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniforge3/lib/python3.10/site-packages (from torchvision==0.22.1) (11.0.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/miniforge3/lib/python3.10/site-packages (from triton==3.3.1->torch==2.7.1) (75.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniforge3/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.7.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniforge3/lib/python3.10/site-packages (from jinja2->torch==2.7.1) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: numpy==1.23.3 in /root/miniforge3/lib/python3.10/site-packages (1.23.3)\n",
      "Requirement already satisfied: pandas in /root/miniforge3/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: triton in /root/miniforge3/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: matplotlib in /root/miniforge3/lib/python3.10/site-packages (3.10.6)\n",
      "Requirement already satisfied: pycuda in /root/miniforge3/lib/python3.10/site-packages (2025.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniforge3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniforge3/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniforge3/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/miniforge3/lib/python3.10/site-packages (from triton) (75.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniforge3/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniforge3/lib/python3.10/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniforge3/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /root/miniforge3/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniforge3/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: pytools>=2011.2 in /root/miniforge3/lib/python3.10/site-packages (from pycuda) (2025.2.4)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /root/miniforge3/lib/python3.10/site-packages (from pycuda) (4.3.6)\n",
      "Requirement already satisfied: mako in /root/miniforge3/lib/python3.10/site-packages (from pycuda) (1.3.10)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: siphash24>=1.6 in /root/miniforge3/lib/python3.10/site-packages (from pytools>=2011.2->pycuda) (1.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /root/miniforge3/lib/python3.10/site-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /root/miniforge3/lib/python3.10/site-packages (from mako->pycuda) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install numpy==1.23.3 pandas triton matplotlib pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Shared Memory per Block: 48.0 KB\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # 自动初始化CUDA上下文\n",
    "\n",
    "# 获取当前设备\n",
    "device = cuda.Context.get_device()\n",
    "\n",
    "# 获取设备属性\n",
    "props = device.get_attributes()\n",
    "\n",
    "# 查看所有属性（会输出一个很长的列表，可以从中找到需要的键）\n",
    "# print(props)\n",
    "\n",
    "# 直接获取关键的共享内存和L2缓存信息\n",
    "# 每个Block（线程块）能使用的最大共享内存量（Bytes）\n",
    "max_shared_memory_per_block = props[cuda.device_attribute.MAX_SHARED_MEMORY_PER_BLOCK]\n",
    "\n",
    "print(f\"Max Shared Memory per Block: {max_shared_memory_per_block / 1024} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def flash_attention(\n",
    "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
    "    seq_len, d_model: tl.constexpr,\n",
    "    stride_qm, stride_km, stride_vm,\n",
    "    BLOCK_M: tl.constexpr,  # Br: Q的分块大小\n",
    "    BLOCK_N: tl.constexpr,  # Bc: K,V的分块大小\n",
    "):\n",
    "    # 只对序列维度进行并行化\n",
    "    pid = tl.program_id(0)\n",
    "    start_m = pid * BLOCK_M\n",
    "    \n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M)\n",
    "    offs_d = tl.arange(0, d_model)  # 处理整个特征维度\n",
    "    \n",
    "    # 初始化状态变量\n",
    "    m_prev = tl.full((BLOCK_M,), float('-inf'), dtype=tl.float32)\n",
    "    l_prev = tl.zeros((BLOCK_M,), dtype=tl.float32)\n",
    "    acc = tl.zeros((BLOCK_M, d_model), dtype=tl.float32)\n",
    "    \n",
    "    # 加载Q分块 (Br × d)\n",
    "    q_mask = offs_m[:, None] < seq_len\n",
    "    q = tl.load(\n",
    "        q_ptr + offs_m[:, None] * stride_qm + offs_d[None, :],\n",
    "        mask=q_mask,\n",
    "        other=0.0\n",
    "    )\n",
    "    \n",
    "    # 按照FlashAttention算法，遍历K,V的分块\n",
    "    for start_n in range(0, seq_len, BLOCK_N):\n",
    "        offs_n = start_n + tl.arange(0, BLOCK_N)\n",
    "        mask_n = offs_n < seq_len\n",
    "        \n",
    "        # 加载K分块 (Bc × d)\n",
    "        k = tl.load(\n",
    "            k_ptr + offs_n[:, None] * stride_km + offs_d[None, :],\n",
    "            mask=mask_n[:, None],\n",
    "            other=0.0\n",
    "        )\n",
    "        \n",
    "        # 加载V分块 (Bc × d)\n",
    "        v = tl.load(\n",
    "            v_ptr + offs_n[:, None] * stride_vm + offs_d[None, :],\n",
    "            mask=mask_n[:, None],\n",
    "            other=0.0\n",
    "        )\n",
    "        \n",
    "        # 计算Sij = Qi @ Kj^T\n",
    "        s = tl.dot(q, k.T)\n",
    "        s *= 1.0 / tl.sqrt(tl.cast(d_model, s.dtype))\n",
    "        \n",
    "        # 掩码无效位置\n",
    "        s = tl.where(mask_n[None, :], s, float('-inf'))\n",
    "        \n",
    "        # 在线Softmax计算(FLASH2)\n",
    "        m_current = tl.maximum(tl.max(s, axis=1), m_prev)\n",
    "        exp_m_prev = tl.exp(m_prev - m_current)\n",
    "        exp_s = tl.exp(s - m_current[:, None])\n",
    "        l_current = exp_m_prev * l_prev + tl.sum(exp_s, axis=1)\n",
    "        \n",
    "        # 更新累加器\n",
    "        acc = acc * exp_m_prev[:, None] + tl.dot(exp_s, v)\n",
    "        \n",
    "        # 更新状态\n",
    "        m_prev = m_current\n",
    "        l_prev = l_current\n",
    "    \n",
    "    # 归一化并写入结果\n",
    "    acc = acc / l_prev[:, None]\n",
    "    tl.store(\n",
    "        o_ptr + offs_m[:, None] * stride_qm + offs_d[None, :],\n",
    "        acc,\n",
    "        mask=q_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_block_sizes(d_model, sram_size_kb=48):\n",
    "    \"\"\"\n",
    "    计算FlashAttention的最优分块大小\n",
    "    \n",
    "    Args:\n",
    "        d_model: 特征维度\n",
    "        sram_size_kb: 共享内存大小（KB）\n",
    "    \n",
    "    Returns:\n",
    "        block_m, block_n: Q和K/V的分块大小\n",
    "    \"\"\"\n",
    "    # 将KB转换为字节\n",
    "    sram_size = sram_size_kb * 1024\n",
    "    \n",
    "    # float32占4字节\n",
    "    bytes_per_elem = 4\n",
    "    \n",
    "    # 根据论文，Bc的计算公式\n",
    "    # 先计算Bc的上界\n",
    "    bc = sram_size // (4 * d_model * bytes_per_elem)\n",
    "    \n",
    "    # 根据论文，Br = min(Bc, d_model)\n",
    "    br = min(bc, d_model)\n",
    "    \n",
    "    # 确保是2的幂\n",
    "    bc = 2 ** (bc.bit_length() - 1) if bc > 0 else 1\n",
    "    \n",
    "    br = min(bc, d_model)\n",
    "    br = 2 ** (br.bit_length() - 1) if br > 0 else 1\n",
    "    \n",
    "    return br, bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_flash_attention(q, k, v, block_m, block_n):\n",
    "    \"\"\"\n",
    "    block_m: Q的分块大小Br\n",
    "    block_n: K,V的分块大小Bc\n",
    "    \"\"\"\n",
    "    assert q.shape == k.shape == v.shape\n",
    "    seq_len, d_model = q.shape\n",
    "\n",
    "    o = torch.empty_like(q)\n",
    "    \n",
    "    # 只在序列维度上并行化\n",
    "    grid = (triton.cdiv(seq_len, block_m),)\n",
    "    \n",
    "    flash_attention[grid](\n",
    "        q, k, v, o,\n",
    "        seq_len, d_model,\n",
    "        q.stride(0), k.stride(0), v.stride(0),\n",
    "        BLOCK_M=block_m,\n",
    "        BLOCK_N=block_n,\n",
    "    )\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK_M (Br): 32\n",
      "BLOCK_N (Bc): 32\n",
      "最大绝对误差: 2.8312206268310547e-07\n",
      "是否近似相等: True\n"
     ]
    }
   ],
   "source": [
    "def torch_attention(q, k, v):\n",
    "    # d_k = d_model\n",
    "    d_k = q.size(-1)\n",
    "    # 为了防止注意力分数方差过大导致softmax梯度消失，需要根号下d_k这个缩放因子\n",
    "    # 方差​​就是​​衡量一组数据与其平均值的偏离程度​\n",
    "    # softmax函数对极端输入值非常敏感\n",
    "    attn_scores = q @ k.transpose(-2, -1) / (d_k ** 0.5) \n",
    "    # 在最后一个维度上进行softmax操作\n",
    "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    return attn_probs @ v \n",
    "\n",
    "# 序列长度\n",
    "seq_len = 1024 \n",
    "# 特征维度\n",
    "d_model = 64\n",
    "\n",
    "# 初始化Q K V输入\n",
    "q = torch.randn(seq_len, d_model, device=\"cuda\", dtype=torch.float32)\n",
    "k = torch.randn_like(q)\n",
    "v = torch.randn_like(q)\n",
    "\n",
    "block_m, block_n = calculate_block_sizes(d_model)\n",
    "    \n",
    "print(f\"BLOCK_M (Br): {block_m}\")\n",
    "print(f\"BLOCK_N (Bc): {block_n}\")\n",
    "\n",
    "# 用 Triton 计算\n",
    "o_triton = call_flash_attention(q, k, v, block_m, block_n)\n",
    "# 用 PyTorch 计算\n",
    "o_torch = torch_attention(q, k, v)\n",
    "\n",
    "print(\"最大绝对误差:\", (o_triton - o_torch).abs().max().item())\n",
    "# 对于两个张量中的每个对应元素都应该满足\n",
    "# |o_triton - o_torch| ≤ atol + rtol × |o_torch|\n",
    "print(\"是否近似相等:\", torch.allclose(o_triton, o_torch, atol=1e-2, rtol=1e-2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
