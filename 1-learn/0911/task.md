AWQ量化 （怎么生成量化参数不重要，具体了解反量化是什么）

> * 困惑度
>
> 困惑度perplexity，简称PPL，评估语言模型性能的一个指标，衡量模型对文本的预测能力。
>
> 困惑度低意味着模型对下一个词的预测比较有把握，困惑度高意味着模型对下一个词的预测比较不确定。
>
> 比如对于句子“今天的天气很__"，如果模型认为填"好"的概率比较高，那困惑度就比较低；如果模型认为填“好"、”坏"、“冷"、”热"的概率差不多，那困惑度就较高。
>
> 困惑度低，只能说明模型预测的准确，不能说明生成的文本质量高。
>
> * 量化和反量化
>
> 量化就是把高精度的浮点数转换成低精度的整数；反量化就是量化的逆过程，即把低精度的整数转换成高精度的浮点数。
>
> 注意，float32->float16不叫量化，叫精度降低/混合精度。
>
> 量化的场景一般有两个，第一个是在模型训练完毕后进行量化(PTQ)，目的是降低模型的存储空间和降低推理时对显存和计算单元的压力；第二个是在模型训练时进行量化和反量化(QAT)，目的是降低PTQ带来的影响，即用更高的训练成本换取更低的推理精度损失。反量化的场景是，在模型进行推理时，如果模型存储的是量化后的，需要进行反量化。另外，模型训练完进行量化，推理时进行反量化，会带来性能损失，这体现在困惑度提高等方面。
>
> 量化包含缩放(scale)和舍入(rounding)两个部分。
>
> * AWQ
>
> AWQ的目的是降低PTQ之后模型在推理时的性能下降问题。它观察到，只有约1%的权重对模型输出有决定性影响。所以它基于激活分布，将那些与高激活值相关的权重通道进行缩放保护(识别重要通道+搜索最优缩放因子)，然后再做量化，在推理时做反量化也需要对重要通道进行相应的缩放性补偿。
>
> 流程像是：权重 -> (放大 s) -> 量化 -> (存储/传输) -> 推理计算 -> 反量化 -> (缩小 /s) -> 正确输出

paged attention 论文 + 网上各种介绍 paged attention的文章，看一看

> * 自回归生成
>
> 自回归生成是一种序列生成方法，其核心特点是逐个生成token，每次生成都依赖于之前已生成的所有token。LLM普遍采用自回归生成。
>
> * KV Cache
>
> 自回归的过程是这样的，每次只生成一个token，然后把新生成的token加入到已有序列后面，作为下一次生成的新输入。这个过程会不断重复，直到生成完整的序列或达到限制的长度。问题在于，每次生成一个token时，都需要处理序列从头到尾所有的token，这其中存在重复计算，如[图](pagedattention/image/learn/1757842158284.png)。
>
> KV Cache的优化方法就是缓存之前计算过的结果，减少重复计算。它是这么做的，它在生成第一个token时，需要得到prompt中每个token的KQV，然后通过注意力算法生成得到第一个token，然后它会将prompt中的每个token的KV缓存起来，在生成下一个token时，只需要得到前一个token的KQV，然后把KV和缓存中的KV拼接起来，去生成下一个token，然后追加缓存中的KV，重复这个过程，如[图](pagedattention/image/learn/1757842136465.png)。
>
> 这相当于是用空间换时间。
>
> * PagedAttention
>
> KV Cache有一个问题，就是内存碎片化严重。在推理时，对于一个prompt，会去申请一块内存来存KV state，一般是生成的最大序列。下面可以分三个方面去分析：1.申请的这些内存，并不是全部都会被用到，即生成完整结果所用的KV state小于预分配的空间，因为我们无法得知最终的序列长度是多少；2.即使我们“神奇地"知道最终KV state所用的空间大小，但在逐个token生成时，尚有部分空间未使用却被提前占用；3.存在外部碎片，各request申请的空间之间存在内存碎片。
>
> 基于以上问题，PagedAttention做了优化，即对KV Cache空间进行分块，每个块包含固定数量的KV state，维护一个块表，记录逻辑块到物理块的映射。KV state以块的形式存储在KV Cache里面，若当前request申请的块都已填满，则再分配一个(动态申请)。这样1.不会存在预申请的空间远大于实际用到的空间；2.在逐个生成token时，不会存在有大量空间已被申请却还未使用；3.不存在外部碎片。

DeepSeek V3用的MLA变种，快速参考：https://www.youtube.com/watch?v=0VLAoVGf_74

FusedMoE 算法

大模型并行化推理中的TP并行模式
